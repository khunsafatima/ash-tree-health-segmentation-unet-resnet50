{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aedb26bd",
   "metadata": {},
   "source": [
    "# train_unet_resnet50_ash_tree_segmentation\n",
    "\n",
    "This notebook trains a UNet segmentation model with a ResNet encoder for multiclass semantic segmentation of RGB ash tree images into:\n",
    "\n",
    "- background\n",
    "- foliage\n",
    "- wood\n",
    "- ivy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a279812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Update these paths for your machine\n",
    "DATA_DIR = Path(\"data\")          # folder containing your .npy arrays\n",
    "OUT_DIR = Path(\"outputs\")        # training logs and saved models\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# These filenames are intentionally left generic.\n",
    "# Provide your own .npy files.\n",
    "X_TRAIN_NPY = \"X_train.npy\"\n",
    "Y_TRAIN_NPY = \"y_train_onehot.npy\"\n",
    "X_VAL_NPY   = \"X_val.npy\"\n",
    "Y_VAL_NPY   = \"y_val_onehot.npy\"\n",
    "\n",
    "# Model setup\n",
    "N_CLASSES = 4\n",
    "IMAGE_SIZE = 128       # expected height and width\n",
    "N_CHANNELS = 3         # RGB\n",
    "\n",
    "BACKBONE = \"resnet50\"  # alternatives: 'resnet34', 'resnet18', etc\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 100\n",
    "LR_INITIAL = 1e-4\n",
    "\n",
    "# Learning rate schedule: if True, switches LR after epoch 50\n",
    "USE_LR_SCHEDULE = True\n",
    "LR_AFTER_EPOCH_50 = 1e-3\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0721c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and environment checks\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# segmentation_models uses keras backend, ensure it sees tf.keras\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "\n",
    "import segmentation_models as sm\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"segmentation_models:\", sm.__version__ if hasattr(sm, \"__version__\") else \"unknown\")\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ae2d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and validation data\n",
    "\n",
    "def load_npy(path: Path) -> np.ndarray:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing file: {path.resolve()}\")\n",
    "    arr = np.load(path)\n",
    "    return arr\n",
    "\n",
    "X_train = load_npy(DATA_DIR / X_TRAIN_NPY)\n",
    "y_train = load_npy(DATA_DIR / Y_TRAIN_NPY)\n",
    "\n",
    "X_val = load_npy(DATA_DIR / X_VAL_NPY)\n",
    "y_val = load_npy(DATA_DIR / Y_VAL_NPY)\n",
    "\n",
    "print(\"X_train:\", X_train.shape, X_train.dtype)\n",
    "print(\"y_train:\", y_train.shape, y_train.dtype)\n",
    "print(\"X_val:\", X_val.shape, X_val.dtype)\n",
    "print(\"y_val:\", y_val.shape, y_val.dtype)\n",
    "\n",
    "# Basic sanity checks\n",
    "assert X_train.ndim == 4 and X_train.shape[-1] == N_CHANNELS, \"X_train must be (N, H, W, 3)\"\n",
    "assert X_train.shape[1] == IMAGE_SIZE and X_train.shape[2] == IMAGE_SIZE, \"Unexpected image size\"\n",
    "assert y_train.ndim == 4 and y_train.shape[-1] == N_CLASSES, \"y_train must be one hot (N, H, W, C)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91285b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess input (encoder specific)\n",
    "\n",
    "preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "\n",
    "X_train_pp = preprocess_input(X_train)\n",
    "X_val_pp = preprocess_input(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60c3320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "\n",
    "model = sm.Unet(\n",
    "    BACKBONE,\n",
    "    input_shape=(IMAGE_SIZE, IMAGE_SIZE, N_CHANNELS),\n",
    "    encoder_weights=\"imagenet\",\n",
    "    classes=N_CLASSES,\n",
    "    activation=\"softmax\",\n",
    ")\n",
    "\n",
    "# Optional: add segmentation friendly metrics\n",
    "metrics = [\n",
    "    \"accuracy\",\n",
    "    sm.metrics.FScore(threshold=None),\n",
    "    sm.metrics.IOUScore(threshold=None),\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LR_INITIAL),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad55d152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "run_name = f\"unet_{BACKBONE}_C{N_CLASSES}_S{IMAGE_SIZE}_BS{BATCH_SIZE}_E{EPOCHS}_LR{LR_INITIAL}\"\n",
    "run_dir = OUT_DIR / run_name\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(str(run_dir / f\"{run_name}.log\"), separator=\",\", append=False)\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=str(run_dir / f\"{run_name}_best.h5\"),\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    ")\n",
    "\n",
    "def lr_scheduler(epoch: int, lr: float) -> float:\n",
    "    if USE_LR_SCHEDULE and epoch >= 50:\n",
    "        return float(LR_AFTER_EPOCH_50)\n",
    "    return float(lr)\n",
    "\n",
    "lr_cb = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "callbacks = [csv_logger, checkpoint, lr_cb, early_stop]\n",
    "print(\"Run directory:\", run_dir.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3d338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit(\n",
    "    X_train_pp,\n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val_pp, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")\n",
    "stop = time.time()\n",
    "\n",
    "print(f\"Training time (seconds): {stop - start:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abc98b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic plots\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(history, out_dir: Path):\n",
    "    hist = history.history\n",
    "\n",
    "    # Loss\n",
    "    plt.figure()\n",
    "    plt.plot(hist.get(\"loss\", []), label=\"train\")\n",
    "    plt.plot(hist.get(\"val_loss\", []), label=\"val\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_dir / \"loss.png\", dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy (if present)\n",
    "    if \"accuracy\" in hist:\n",
    "        plt.figure()\n",
    "        plt.plot(hist.get(\"accuracy\", []), label=\"train\")\n",
    "        plt.plot(hist.get(\"val_accuracy\", []), label=\"val\")\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_dir / \"accuracy.png\", dpi=200)\n",
    "        plt.show()\n",
    "\n",
    "plot_history(history, run_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568388ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model (without optimiser state to reduce file size)\n",
    "\n",
    "final_path = run_dir / f\"{run_name}_final.h5\"\n",
    "model.save(final_path, include_optimizer=False)\n",
    "print(\"Saved:\", final_path.resolve())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (latest Jaspy)",
   "language": "python",
   "name": "jaspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
